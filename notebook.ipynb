{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "# Carga de datos\n",
    "#\n",
    "\n",
    "def load_data():\n",
    "\n",
    "    import pandas as pd\n",
    "\n",
    "    url = \"http://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\"\n",
    "    df = pd.read_csv(url, sep=\";\")\n",
    "\n",
    "    y = df[\"quality\"]\n",
    "    x = df.copy()\n",
    "    x.pop(\"quality\")\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Particionamiento de datos\n",
    "#\n",
    "def make_train_test_split(x, y):\n",
    "\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    (x_train, x_test, y_train, y_test) = train_test_split(\n",
    "        x,\n",
    "        y,\n",
    "        test_size=0.25,\n",
    "        random_state=0,\n",
    "    )\n",
    "    return x_train, x_test, y_train, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Cálculo de metricas de evaluación\n",
    "#\n",
    "def eval_metrics(y_true, y_pred):\n",
    "\n",
    "    from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "\n",
    "    return mse, mae, r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#\n",
    "# Reporte de métricas de evaluación\n",
    "#\n",
    "def report(estimator, mse, mae, r2):\n",
    "\n",
    "    print(estimator, \":\", sep=\"\")\n",
    "    print(f\"  MSE: {mse}\")\n",
    "    print(f\"  MAE: {mae}\")\n",
    "    print(f\"  R2: {r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Almacenamiento del modelo\n",
    "#\n",
    "def save_best_estimator(estimator):\n",
    "\n",
    "    import os\n",
    "    import pickle\n",
    "\n",
    "    with open(\"estimator.pickle\", \"wb\") as file:\n",
    "        pickle.dump(estimator, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Carga del modelo\n",
    "#\n",
    "def load_best_estimator():\n",
    "\n",
    "    import os\n",
    "    import pickle\n",
    "\n",
    "    if not os.path.exists(\"estimator.pickle\"):\n",
    "        return None\n",
    "    with open(\"estimator.pickle\", \"rb\") as file:\n",
    "        estimator = pickle.load(file)\n",
    "\n",
    "    return estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Entrenamiento\n",
    "#\n",
    "def train_estimator(alpha=0.5, l1_ratio=0.5, verbose=1):\n",
    "\n",
    "    from sklearn.linear_model import ElasticNet\n",
    "\n",
    "    x, y = load_data()\n",
    "    x_train, x_test, y_train, y_test = make_train_test_split(x, y)\n",
    "    estimator = ElasticNet(alpha=alpha, l1_ratio=l1_ratio, random_state=12345)\n",
    "    estimator.fit(x_train, y_train)\n",
    "    mse, mae, r2 = eval_metrics(y_test, y_pred=estimator.predict(x_test))\n",
    "    if verbose > 0:\n",
    "        report(estimator, mse, mae, r2)\n",
    "\n",
    "    best_estimator = load_best_estimator()\n",
    "    if best_estimator is None or estimator.score(x_test, y_test) > best_estimator.score(\n",
    "        x_test, y_test\n",
    "    ):\n",
    "        best_estimator = estimator\n",
    "\n",
    "    save_best_estimator(best_estimator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_estimator(0.5, 0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_estimator(0.5, 0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_estimator(0.2, 0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_estimator(0.1, 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_estimator():\n",
    "\n",
    "    x, y = load_data()\n",
    "    x_train, x_test, y_train, y_test = make_train_test_split(x, y)\n",
    "    estimator = load_best_estimator()\n",
    "    mse, mae, r2 = eval_metrics(y_test, y_pred=estimator.predict(x_test))\n",
    "    report(estimator, mse, mae, r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Debe coincidir con el mejor modelo encontrado en la celdas anteriores\n",
    "#\n",
    "check_estimator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_hyperparameter_search(alphas, l1_ratios):\n",
    "    \n",
    "        for alpha in alphas:\n",
    "            for l1_ratio in l1_ratios:\n",
    "                train_estimator(alpha, l1_ratio, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "alphas = np.linspace(0.0001, 0.5, 10)\n",
    "l1_ratios = np.linspace(0.0001, 0.5, 10)\n",
    "make_hyperparameters_search(alphas, l1_ratios)\n",
    "check_estimator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_estimator(alphas, l1_ratios, n_splits=5, verbose=1):\n",
    "\n",
    "    from sklearn.linear_model import ElasticNet\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "    x, y = load_data()\n",
    "    x_train, x_test, y_train, y_test = make_train_test_split(x, y)\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Búsqueda de parámetros con validación cruzada\n",
    "    #\n",
    "    estimator = GridSearchCV(\n",
    "        estimator=ElasticNet(\n",
    "            random_state=12345,\n",
    "        ),\n",
    "        param_grid={\n",
    "            \"alpha\": alphas,\n",
    "            \"l1_ratio\": l1_ratios,\n",
    "        },\n",
    "        cv=n_splits,\n",
    "        refit=True,\n",
    "        verbose=0,\n",
    "        return_train_score=False,\n",
    "    )\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    estimator.fit(x_train, y_train)\n",
    "\n",
    "    estimator = estimator.best_estimator_\n",
    "\n",
    "    mse, mae, r2 = eval_metrics(y_test, y_pred=estimator.predict(x_test))\n",
    "    if verbose > 0:\n",
    "        report(estimator, mse, mae, r2)\n",
    "\n",
    "    best_estimator = load_best_estimator()\n",
    "    if best_estimator is None or estimator.score(x_test, y_test) > best_estimator.score(\n",
    "        x_test, y_test\n",
    "    ):\n",
    "        best_estimator = estimator\n",
    "\n",
    "    save_best_estimator(best_estimator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "train_estimator(\n",
    "    alphas=np.linspace(0.0001, 0.5, 10),\n",
    "    l1_ratios=np.linspace(0.0001, 0.5, 10),\n",
    "    n_splits=5,\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_estimator():\n",
    "\n",
    "    x, y = load_data()\n",
    "    x_train, x_test, y_train, y_test = make_train_test_split(x, y)\n",
    "    estimator = load_best_estimator()\n",
    "    mse, mae, r2 = eval_metrics(y_test, y_pred=estimator.predict(x_test))\n",
    "    report(estimator, mse, mae, r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Debe coincidir con el mejor modelo encontrado en la celdas anteriores\n",
    "#\n",
    "check_estimator()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
